{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":2228850,"sourceType":"datasetVersion","datasetId":1338904}],"dockerImageVersionId":31013,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Tuple\nimport os\nfrom tqdm import tqdm\nimport re\nimport json\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nclass ArabicGPTTPU:\n    def __init__(self, model_name: str = \"aubmindlab/aragpt2-base\"):\n        \"\"\"\n        Initialize the Arabic GPT model with TPU support\n        Args:\n            model_name: Name of the pre-trained model to use\n        \"\"\"\n        # Initialize TPU device\n        self.device = xm.xla_device()\n        print(f\"Using TPU device: {self.device}\")\n\n        # Initialize tokenizer and model\n        print(\"Loading tokenizer and model...\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n\n        # Add special tokens for Arabic\n        special_tokens = {\n            'pad_token': '[PAD]',\n            'bos_token': '[BOS]',\n            'eos_token': '[EOS]',\n            'unk_token': '[UNK]'\n        }\n        self.tokenizer.add_special_tokens(special_tokens)\n        self.model.resize_token_embeddings(len(self.tokenizer))\n\n        # Move model to TPU\n        self.model = self.model.to(self.device)\n\n    def save_model(self, save_dir: str = 'arabic_gpt_tpu_model'):\n        \"\"\"\n        Save the model and tokenizer to disk\n        Args:\n            save_dir: Directory to save the model and tokenizer\n        \"\"\"\n        print(f\"Saving model to {save_dir}...\")\n\n        # Create directory if it doesn't exist\n        os.makedirs(save_dir, exist_ok=True)\n\n        # Move model to CPU before saving\n        self.model = self.model.to('cpu')\n\n        # Save model\n        model_path = os.path.join(save_dir, 'model')\n        self.model.save_pretrained(model_path)\n\n        # Save tokenizer\n        tokenizer_path = os.path.join(save_dir, 'tokenizer')\n        self.tokenizer.save_pretrained(tokenizer_path)\n\n        # Save model configuration\n        config = {\n            'model_name': self.model.config.model_type,\n            'vocab_size': self.model.config.vocab_size,\n            'max_position_embeddings': self.model.config.max_position_embeddings,\n            'num_attention_heads': self.model.config.num_attention_heads,\n            'num_hidden_layers': self.model.config.num_hidden_layers,\n        }\n\n        with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n            json.dump(config, f, indent=4)\n\n        print(\"Model saved successfully!\")\n\n        # Move model back to TPU\n        self.model = self.model.to(self.device)\n\n    @classmethod\n    def load_model(cls, load_dir: str = 'arabic_gpt_tpu_model'):\n        \"\"\"\n        Load a saved model and tokenizer\n        Args:\n            load_dir: Directory containing the saved model and tokenizer\n        Returns:\n            ArabicGPTTPU instance with loaded model\n        \"\"\"\n        print(f\"Loading model from {load_dir}...\")\n\n        # Create instance\n        instance = cls()\n\n        # Load model\n        model_path = os.path.join(load_dir, 'model')\n        instance.model = AutoModelForCausalLM.from_pretrained(model_path)\n        instance.model = instance.model.to(instance.device)\n\n        # Load tokenizer\n        tokenizer_path = os.path.join(load_dir, 'tokenizer')\n        instance.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n\n        print(\"Model loaded successfully!\")\n        return instance\n\n    def clean_text(self, text: str) -> str:\n        \"\"\"\n        Clean and normalize Arabic text\n        \"\"\"\n        # Remove URLs\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n        # Remove special characters and numbers\n        text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n        # Remove extra spaces\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n\n    def preprocess_data(self, file_path: str = '/kaggle/input/arabic-news/Arabic_news.csv') -> List[str]:\n        \"\"\"\n        Preprocess the Arabic news dataset\n        Args:\n            file_path: Path to the dataset file\n        Returns:\n            List of preprocessed sentences\n        \"\"\"\n        print(f\"Loading dataset from {file_path}\")\n        df = pd.read_csv(file_path)\n\n        # Clean and preprocess the text\n        print(\"Preprocessing text data...\")\n        df=df[:30000]\n        df['text'] = df['text'].fillna('')\n        df['text'] = df['text'].apply(self.clean_text)\n        df = df[df['text'].str.len() > 0]\n\n        # Split into sentences and create training pairs\n        sentences = []\n        for text in tqdm(df['text'], desc=\"Processing sentences\"):\n            # Split by common Arabic sentence endings\n            text_sentences = [s.strip() for s in text.split('۔') if len(s.strip()) > 0]\n            # Add BOS and EOS tokens\n            text_sentences = [f\"[BOS] {s} [EOS]\" for s in text_sentences]\n            sentences.extend(text_sentences)\n\n        print(f\"Total number of sentences: {len(sentences)}\")\n        return sentences\n\n    def prepare_batch(self, sentences: List[str], batch_size: int) -> torch.Tensor:\n        \"\"\"\n        Prepare a batch of sentences for training\n        \"\"\"\n        encodings = self.tokenizer(sentences,\n                                 padding=True,\n                                 truncation=True,\n                                 max_length=128,  # Reduced from 512 to save memory\n                                 return_tensors=\"pt\")\n        return encodings\n\n    def train(self, sentences: List[str], epochs: int = 3, batch_size: int = 8, gradient_accumulation_steps: int = 4):\n        \"\"\"\n        Fine-tune the model on the Arabic news dataset using TPU with memory-efficient training\n        Args:\n            sentences: List of training sentences\n            epochs: Number of training epochs\n            batch_size: Batch size for training (reduced for TPU memory constraints)\n            gradient_accumulation_steps: Number of steps to accumulate gradients\n        \"\"\"\n        self.model.train()\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)\n\n        # Create dataset\n        encodings = self.prepare_batch(sentences, batch_size)\n        dataset = torch.utils.data.TensorDataset(\n            encodings[\"input_ids\"],\n            encodings[\"attention_mask\"]\n        )\n\n        # Create TPU-optimized data loader with smaller batch size\n        train_loader = pl.ParallelLoader(\n            torch.utils.data.DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=True\n            ),\n            [self.device]\n        ).per_device_loader(self.device)\n\n        # Calculate effective batch size\n        effective_batch_size = batch_size * gradient_accumulation_steps\n\n        for epoch in range(epochs):\n            total_loss = 0\n            optimizer.zero_grad()\n            running_loss = 0\n            num_batches = 0\n\n            # Reset the data loader for each epoch\n            train_loader = pl.ParallelLoader(\n                torch.utils.data.DataLoader(\n                    dataset,\n                    batch_size=batch_size,\n                    shuffle=True\n                ),\n                [self.device]\n            ).per_device_loader(self.device)\n\n            # Calculate total number of batches for this epoch\n            total_batches = len(train_loader)\n            mid_point = total_batches // 2\n\n            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")):\n                input_ids, attention_mask = batch\n\n                # Forward pass\n                outputs = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=input_ids\n                )\n                loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n\n                # Backward pass\n                loss.backward()\n\n                # Update weights if we've accumulated enough gradients\n                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n                    # TPU optimizer step\n                    xm.optimizer_step(optimizer)\n                    optimizer.zero_grad()\n\n                    # Mark step for TPU\n                    xm.mark_step()\n\n                # Update running loss\n                running_loss += loss.item() * gradient_accumulation_steps\n                num_batches += 1\n\n                # Print progress at middle of epoch\n                if batch_idx == mid_point:\n                    avg_loss = running_loss / num_batches\n                    xm.master_print(f\"\\nEpoch {epoch + 1}/{epochs} - Midpoint, Average Loss: {avg_loss:.4f}\\n\")\n\n            # Calculate and print final epoch average loss\n            epoch_avg_loss = running_loss / num_batches\n            xm.master_print(f\"\\nEpoch {epoch + 1}/{epochs} completed, Final Average Loss: {epoch_avg_loss:.4f}\\n\")\n\n            # Save checkpoint after each epoch\n            self.save_model(f'arabic_gpt_tpu_model_epoch_{epoch + 1}')\n\n    def predict_next_word(self, sentence: str, num_predictions: int = 5) -> List[Tuple[str, float]]:\n        \"\"\"\n        Predict the next word given a sentence\n        Args:\n            sentence: Input sentence\n            num_predictions: Number of predictions to return\n        Returns:\n            List of tuples containing (predicted_word, probability)\n        \"\"\"\n        # Set model to eval mode once\n        self.model.eval()\n\n        with torch.no_grad():\n            # Add BOS token to input\n            input_text = f\"[BOS] {sentence}\"\n            inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n            input_ids = inputs[\"input_ids\"].to(self.device)\n            attention_mask = inputs[\"attention_mask\"].to(self.device)\n\n            # Get logits for the last position only\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=False,\n                output_attentions=False\n            )\n            last_token_logits = outputs.logits[0, -1, :]\n\n            # Get top k predictions\n            top_k_logits, top_k_indices = torch.topk(last_token_logits, num_predictions)\n            probs = torch.softmax(top_k_logits, dim=-1)\n\n            # Convert to list of predictions\n            predictions = []\n            for idx, (token_id, prob) in enumerate(zip(top_k_indices, probs)):\n                word = self.tokenizer.decode([token_id])\n                predictions.append((word, prob.item()))\n\n            return predictions\n\ndef main():\n    # Initialize the model\n    print(\"Initializing model...\")\n    model = ArabicGPTTPU()\n\n    # Load and preprocess data\n    print(\"Loading and preprocessing data...\")\n    sentences = model.preprocess_data()\n\n    # Train the model with memory-efficient settings\n    print(\"Training the model...\")\n    model.train(\n        sentences,\n        epochs=10,\n        batch_size=8,  # Reduced batch size\n        gradient_accumulation_steps=4  # Added gradient accumulation\n    )\n\n    # Example usage with the trained model\n    print(\"\\nTesting predictions with the trained model:\")\n    test_sentences = [\n        \"مرحبا كيف حالك\",\n        \"انا اريد ان اتعلم\",\n        \"شكرا جزيلا على\",\n        \"السلام عليكم ورحمة\",\n        \"اهلا وسهلا بكم في\",\n        \"انا احب القراءة\",\n        \"هذا الكتاب جميل\"\n    ]\n\n    for test_sentence in test_sentences:\n        predictions = model.predict_next_word(test_sentence)\n        print(f\"\\nInput sentence: {test_sentence}\")\n        print(\"Predictions:\")\n        for word, prob in predictions:\n            print(f\"{word}: {prob:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87kHCF7VQipy","outputId":"4421057c-a102-406a-d30f-9e0c621f338b","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T17:01:46.754761Z","iopub.execute_input":"2025-05-11T17:01:46.754961Z","iopub.status.idle":"2025-05-11T18:30:47.486587Z","shell.execute_reply.started":"2025-05-11T17:01:46.754940Z","shell.execute_reply":"2025-05-11T18:30:47.481645Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:251: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Initializing model...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1746982925.701381     299 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:232\n","output_type":"stream"},{"name":"stdout","text":"Using TPU device: xla:0\nLoading tokenizer and model...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1746982939.930728     299 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:230\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nWARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nThe new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"Loading and preprocessing data...\nLoading dataset from /kaggle/input/arabic-news/Arabic_news.csv\nPreprocessing text data...\n","output_type":"stream"},{"name":"stderr","text":"Processing sentences: 100%|██████████| 30000/30000 [00:00<00:00, 265329.61it/s]\n","output_type":"stream"},{"name":"stdout","text":"Total number of sentences: 30000\nTraining the model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10:   0%|          | 0/3750 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nEpoch 1/10:  50%|█████     | 1875/3750 [06:27<04:51,  6.43it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10 - Midpoint, Average Loss: 5.0612\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 3750/3750 [11:51<00:00,  5.27it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/10 completed, Final Average Loss: 4.6538\n\nSaving model to arabic_gpt_tpu_model_epoch_1...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10:  50%|█████     | 1875/3750 [04:09<03:45,  8.33it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10 - Midpoint, Average Loss: 4.0368\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 3750/3750 [08:02<00:00,  7.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/10 completed, Final Average Loss: 4.0393\n\nSaving model to arabic_gpt_tpu_model_epoch_2...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10:  50%|█████     | 1875/3750 [03:53<03:40,  8.49it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/10 - Midpoint, Average Loss: 4.0512\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 3750/3750 [07:46<00:00,  8.04it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/10 completed, Final Average Loss: 4.0400\n\nSaving model to arabic_gpt_tpu_model_epoch_3...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10:  50%|█████     | 1875/3750 [03:54<03:52,  8.08it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/10 - Midpoint, Average Loss: 4.0423\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 3750/3750 [08:03<00:00,  7.75it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/10 completed, Final Average Loss: 4.0396\n\nSaving model to arabic_gpt_tpu_model_epoch_4...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10:  50%|█████     | 1875/3750 [04:33<04:37,  6.77it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/10 - Midpoint, Average Loss: 4.0357\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 3750/3750 [09:14<00:00,  6.77it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/10 completed, Final Average Loss: 4.0392\n\nSaving model to arabic_gpt_tpu_model_epoch_5...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10:  50%|█████     | 1875/3750 [04:28<03:49,  8.17it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10 - Midpoint, Average Loss: 4.0402\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 3750/3750 [08:19<00:00,  7.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/10 completed, Final Average Loss: 4.0398\n\nSaving model to arabic_gpt_tpu_model_epoch_6...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10:  50%|█████     | 1875/3750 [03:56<03:50,  8.13it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/10 - Midpoint, Average Loss: 4.0370\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 3750/3750 [07:48<00:00,  8.00it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/10 completed, Final Average Loss: 4.0395\n\nSaving model to arabic_gpt_tpu_model_epoch_7...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10:  50%|█████     | 1875/3750 [03:53<03:42,  8.41it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/10 - Midpoint, Average Loss: 4.0425\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 3750/3750 [07:46<00:00,  8.03it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/10 completed, Final Average Loss: 4.0399\n\nSaving model to arabic_gpt_tpu_model_epoch_8...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10:  50%|█████     | 1875/3750 [03:51<03:43,  8.40it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/10 - Midpoint, Average Loss: 4.0356\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 3750/3750 [07:43<00:00,  8.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/10 completed, Final Average Loss: 4.0399\n\nSaving model to arabic_gpt_tpu_model_epoch_9...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10:  50%|█████     | 1875/3750 [03:52<03:40,  8.51it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/10 - Midpoint, Average Loss: 4.0407\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 3750/3750 [07:44<00:00,  8.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/10 completed, Final Average Loss: 4.0397\n\nSaving model to arabic_gpt_tpu_model_epoch_10...\nModel saved successfully!\n\nTesting predictions with the trained model:\n\nInput sentence: مرحبا كيف حالك\nPredictions:\n يا: 0.7270\n في: 0.0914\n إذا: 0.0724\n ؟: 0.0597\n مع: 0.0495\n\nInput sentence: انا اريد ان اتعلم\nPredictions:\n من: 0.3163\n شيئا: 0.2006\n كيف: 0.1968\n الفرنسية: 0.1552\n اللغة: 0.1312\n\nInput sentence: شكرا جزيلا على\nPredictions:\n هذا: 0.3313\n جهود: 0.2204\n هذه: 0.2106\n ما: 0.1247\n كل: 0.1130\n\nInput sentence: السلام عليكم ورحمة\nPredictions:\n الله: 0.9881\n الرحمن: 0.0054\n الباري: 0.0026\n لله: 0.0023\n ربي: 0.0017\n\nInput sentence: اهلا وسهلا بكم في\nPredictions:\n هذا: 0.3324\n موقع: 0.2732\n منتدى: 0.1655\n هذه: 0.1581\n منتديات: 0.0707\n\nInput sentence: انا احب القراءة\nPredictions:\n كثيرا: 0.3755\n�: 0.2477\n في: 0.1619\n والكتابة: 0.1165\n منذ: 0.0984\n\nInput sentence: هذا الكتاب جميل\nPredictions:\n جدا: 0.5786\n في: 0.2053\n من: 0.1052\n للغاية: 0.0615\n ومفيد: 0.0494\n","output_type":"stream"}],"execution_count":1}]}