{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"V28"},"accelerator":"TPU","kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":2228850,"sourceType":"datasetVersion","datasetId":1338904}],"dockerImageVersionId":31013,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport pandas as pd\nimport numpy as np\nfrom typing import List, Tuple\nimport os\nfrom tqdm import tqdm\nimport re\nimport json\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nclass ArabicGPTTPU:\n    def __init__(self, model_name: str = \"aubmindlab/aragpt2-base\"):\n        \"\"\"\n        Initialize the Arabic GPT model with TPU support\n        Args:\n            model_name: Name of the pre-trained model to use\n        \"\"\"\n        # Initialize TPU device\n        self.device = xm.xla_device()\n        print(f\"Using TPU device: {self.device}\")\n\n        # Initialize tokenizer and model\n        print(\"Loading tokenizer and model...\")\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n\n        # Add special tokens for Arabic\n        special_tokens = {\n            'pad_token': '[PAD]',\n            'bos_token': '[BOS]',\n            'eos_token': '[EOS]',\n            'unk_token': '[UNK]'\n        }\n        self.tokenizer.add_special_tokens(special_tokens)\n        self.model.resize_token_embeddings(len(self.tokenizer))\n\n        # Move model to TPU\n        self.model = self.model.to(self.device)\n\n    def save_model(self, save_dir: str = 'arabic_gpt_tpu_model'):\n        \"\"\"\n        Save the model and tokenizer to disk\n        Args:\n            save_dir: Directory to save the model and tokenizer\n        \"\"\"\n        print(f\"Saving model to {save_dir}...\")\n\n        # Create directory if it doesn't exist\n        os.makedirs(save_dir, exist_ok=True)\n\n        # Move model to CPU before saving\n        self.model = self.model.to('cpu')\n\n        # Save model\n        model_path = os.path.join(save_dir, 'model')\n        self.model.save_pretrained(model_path)\n\n        # Save tokenizer\n        tokenizer_path = os.path.join(save_dir, 'tokenizer')\n        self.tokenizer.save_pretrained(tokenizer_path)\n\n        # Save model configuration\n        config = {\n            'model_name': self.model.config.model_type,\n            'vocab_size': self.model.config.vocab_size,\n            'max_position_embeddings': self.model.config.max_position_embeddings,\n            'num_attention_heads': self.model.config.num_attention_heads,\n            'num_hidden_layers': self.model.config.num_hidden_layers,\n        }\n\n        with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n            json.dump(config, f, indent=4)\n\n        print(\"Model saved successfully!\")\n\n        # Move model back to TPU\n        self.model = self.model.to(self.device)\n\n    @classmethod\n    def load_model(cls, load_dir: str = 'arabic_gpt_tpu_model'):\n        \"\"\"\n        Load a saved model and tokenizer\n        Args:\n            load_dir: Directory containing the saved model and tokenizer\n        Returns:\n            ArabicGPTTPU instance with loaded model\n        \"\"\"\n        print(f\"Loading model from {load_dir}...\")\n\n        # Create instance\n        instance = cls()\n\n        # Load model\n        model_path = os.path.join(load_dir, 'model')\n        instance.model = AutoModelForCausalLM.from_pretrained(model_path)\n        instance.model = instance.model.to(instance.device)\n\n        # Load tokenizer\n        tokenizer_path = os.path.join(load_dir, 'tokenizer')\n        instance.tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)\n\n        print(\"Model loaded successfully!\")\n        return instance\n\n    def clean_text(self, text: str) -> str:\n        \"\"\"\n        Clean and normalize Arabic text\n        \"\"\"\n        # Remove URLs\n        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n        # Remove special characters and numbers\n        text = re.sub(r'[^\\u0600-\\u06FF\\s]', '', text)\n        # Remove extra spaces\n        text = re.sub(r'\\s+', ' ', text).strip()\n        return text\n\n    def preprocess_data(self, file_path: str = '/kaggle/input/arabic-news/Arabic_news.csv') -> List[str]:\n        \"\"\"\n        Preprocess the Arabic news dataset\n        Args:\n            file_path: Path to the dataset file\n        Returns:\n            List of preprocessed sentences\n        \"\"\"\n        print(f\"Loading dataset from {file_path}\")\n        df = pd.read_csv(file_path)\n\n        # Clean and preprocess the text\n        print(\"Preprocessing text data...\")\n        df=df[:15000]\n        df['text'] = df['text'].fillna('')\n        df['text'] = df['text'].apply(self.clean_text)\n        df = df[df['text'].str.len() > 0]\n\n        # Split into sentences and create training pairs\n        sentences = []\n        for text in tqdm(df['text'], desc=\"Processing sentences\"):\n            # Split by common Arabic sentence endings\n            text_sentences = [s.strip() for s in text.split('۔') if len(s.strip()) > 0]\n            # Add BOS and EOS tokens\n            text_sentences = [f\"[BOS] {s} [EOS]\" for s in text_sentences]\n            sentences.extend(text_sentences)\n\n        print(f\"Total number of sentences: {len(sentences)}\")\n        return sentences\n\n    def prepare_batch(self, sentences: List[str], batch_size: int) -> torch.Tensor:\n        \"\"\"\n        Prepare a batch of sentences for training\n        \"\"\"\n        encodings = self.tokenizer(sentences,\n                                 padding=True,\n                                 truncation=True,\n                                 max_length=128,  # Reduced from 512 to save memory\n                                 return_tensors=\"pt\")\n        return encodings\n\n    def train(self, sentences: List[str], epochs: int = 3, batch_size: int = 8, gradient_accumulation_steps: int = 4):\n        \"\"\"\n        Fine-tune the model on the Arabic news dataset using TPU with memory-efficient training\n        Args:\n            sentences: List of training sentences\n            epochs: Number of training epochs\n            batch_size: Batch size for training (reduced for TPU memory constraints)\n            gradient_accumulation_steps: Number of steps to accumulate gradients\n        \"\"\"\n        self.model.train()\n        optimizer = torch.optim.AdamW(self.model.parameters(), lr=5e-5)\n\n        # Create dataset\n        encodings = self.prepare_batch(sentences, batch_size)\n        dataset = torch.utils.data.TensorDataset(\n            encodings[\"input_ids\"],\n            encodings[\"attention_mask\"]\n        )\n\n        # Create TPU-optimized data loader with smaller batch size\n        train_loader = pl.ParallelLoader(\n            torch.utils.data.DataLoader(\n                dataset,\n                batch_size=batch_size,\n                shuffle=True\n            ),\n            [self.device]\n        ).per_device_loader(self.device)\n\n        # Calculate effective batch size\n        effective_batch_size = batch_size * gradient_accumulation_steps\n\n        for epoch in range(epochs):\n            total_loss = 0\n            optimizer.zero_grad()\n            running_loss = 0\n            num_batches = 0\n\n            # Reset the data loader for each epoch\n            train_loader = pl.ParallelLoader(\n                torch.utils.data.DataLoader(\n                    dataset,\n                    batch_size=batch_size,\n                    shuffle=True\n                ),\n                [self.device]\n            ).per_device_loader(self.device)\n\n            # Calculate total number of batches for this epoch\n            total_batches = len(train_loader)\n            mid_point = total_batches // 2\n\n            for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\")):\n                input_ids, attention_mask = batch\n\n                # Forward pass\n                outputs = self.model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=input_ids\n                )\n                loss = outputs.loss / gradient_accumulation_steps  # Normalize loss\n\n                # Backward pass\n                loss.backward()\n\n                # Update weights if we've accumulated enough gradients\n                if (batch_idx + 1) % gradient_accumulation_steps == 0:\n                    # TPU optimizer step\n                    xm.optimizer_step(optimizer)\n                    optimizer.zero_grad()\n\n                    # Mark step for TPU\n                    xm.mark_step()\n\n                # Update running loss\n                running_loss += loss.item() * gradient_accumulation_steps\n                num_batches += 1\n\n                # Print progress at middle of epoch\n                if batch_idx == mid_point:\n                    avg_loss = running_loss / num_batches\n                    xm.master_print(f\"\\nEpoch {epoch + 1}/{epochs} - Midpoint, Average Loss: {avg_loss:.4f}\\n\")\n\n            # Calculate and print final epoch average loss\n            epoch_avg_loss = running_loss / num_batches\n            xm.master_print(f\"\\nEpoch {epoch + 1}/{epochs} completed, Final Average Loss: {epoch_avg_loss:.4f}\\n\")\n            if (epoch+1)%5==0:\n            # Save checkpoint after each epoch\n                self.save_model(f'arabic_gpt_tpu_model_epoch_{epoch + 1}')\n\n    def predict_next_word(self, sentence: str, num_predictions: int = 5) -> List[Tuple[str, float]]:\n        \"\"\"\n        Predict the next word given a sentence\n        Args:\n            sentence: Input sentence\n            num_predictions: Number of predictions to return\n        Returns:\n            List of tuples containing (predicted_word, probability)\n        \"\"\"\n        # Set model to eval mode once\n        self.model.eval()\n\n        with torch.no_grad():\n            # Add BOS token to input\n            input_text = f\"[BOS] {sentence}\"\n            inputs = self.tokenizer(input_text, return_tensors=\"pt\")\n            input_ids = inputs[\"input_ids\"].to(self.device)\n            attention_mask = inputs[\"attention_mask\"].to(self.device)\n\n            # Get logits for the last position only\n            outputs = self.model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                output_hidden_states=False,\n                output_attentions=False\n            )\n            last_token_logits = outputs.logits[0, -1, :]\n\n            # Get top k predictions\n            top_k_logits, top_k_indices = torch.topk(last_token_logits, num_predictions)\n            probs = torch.softmax(top_k_logits, dim=-1)\n\n            # Convert to list of predictions\n            predictions = []\n            for idx, (token_id, prob) in enumerate(zip(top_k_indices, probs)):\n                word = self.tokenizer.decode([token_id])\n                predictions.append((word, prob.item()))\n\n            return predictions\n\ndef main():\n    # Initialize the model\n    print(\"Initializing model...\")\n    model = ArabicGPTTPU()\n\n    # Load and preprocess data\n    print(\"Loading and preprocessing data...\")\n    sentences = model.preprocess_data()\n\n    # Train the model with memory-efficient settings\n    print(\"Training the model...\")\n    model.train(\n        sentences,\n        epochs=30,\n        batch_size=8,  # Reduced batch size\n        gradient_accumulation_steps=4  # Added gradient accumulation\n    )\n\n    # Example usage with the trained model\n    print(\"\\nTesting predictions with the trained model:\")\n    test_sentences = [\n        \"مرحبا كيف حالك\",\n        \"انا اريد ان اتعلم\",\n        \"شكرا جزيلا على\",\n        \"السلام عليكم ورحمة\",\n        \"اهلا وسهلا بكم في\",\n        \"انا احب القراءة\",\n        \"هذا الكتاب جميل\"\n    ]\n\n    for test_sentence in test_sentences:\n        predictions = model.predict_next_word(test_sentence)\n        print(f\"\\nInput sentence: {test_sentence}\")\n        print(\"Predictions:\")\n        for word, prob in predictions:\n            print(f\"{word}: {prob:.4f}\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87kHCF7VQipy","outputId":"4421057c-a102-406a-d30f-9e0c621f338b","trusted":true,"execution":{"iopub.status.busy":"2025-05-11T19:52:19.706961Z","iopub.execute_input":"2025-05-11T19:52:19.707204Z","iopub.status.idle":"2025-05-11T22:12:03.257667Z","shell.execute_reply.started":"2025-05-11T19:52:19.707179Z","shell.execute_reply":"2025-05-11T22:12:03.252305Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:251: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Initializing model...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1746993159.515732     297 common_lib.cc:621] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:232\n","output_type":"stream"},{"name":"stdout","text":"Using TPU device: xla:0\nLoading tokenizer and model...\n","output_type":"stream"},{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1746993173.211080     297 common_lib.cc:612] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: ===\nlearning/45eac/tfrc/runtime/common_lib.cc:230\nXet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nWARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\nThe new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"},{"name":"stdout","text":"Loading and preprocessing data...\nLoading dataset from /kaggle/input/arabic-news/Arabic_news.csv\nPreprocessing text data...\n","output_type":"stream"},{"name":"stderr","text":"Processing sentences: 100%|██████████| 15000/15000 [00:00<00:00, 273231.51it/s]\n","output_type":"stream"},{"name":"stdout","text":"Total number of sentences: 15000\nTraining the model...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/30:   0%|          | 0/1875 [00:00<?, ?it/s]`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\nEpoch 1/30:  50%|████▉     | 936/1875 [03:17<03:04,  5.09it/s] ","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/30 - Midpoint, Average Loss: 5.5270\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/30: 100%|██████████| 1875/1875 [05:51<00:00,  5.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 1/30 completed, Final Average Loss: 5.0689\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30:  50%|████▉     | 936/1875 [02:33<02:58,  5.26it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/30 - Midpoint, Average Loss: 4.2404\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/30: 100%|██████████| 1875/1875 [05:07<00:00,  6.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 2/30 completed, Final Average Loss: 4.1808\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30:  50%|████▉     | 936/1875 [02:33<02:54,  5.37it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/30 - Midpoint, Average Loss: 3.9407\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/30: 100%|██████████| 1875/1875 [05:07<00:00,  6.09it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 3/30 completed, Final Average Loss: 3.9220\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30:  50%|████▉     | 936/1875 [02:33<03:02,  5.15it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/30 - Midpoint, Average Loss: 3.7573\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/30: 100%|██████████| 1875/1875 [05:06<00:00,  6.12it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 4/30 completed, Final Average Loss: 3.7476\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30:  50%|████▉     | 936/1875 [02:34<02:58,  5.25it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/30 - Midpoint, Average Loss: 3.6086\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/30: 100%|██████████| 1875/1875 [05:08<00:00,  6.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 5/30 completed, Final Average Loss: 3.6062\n\nSaving model to arabic_gpt_tpu_model_epoch_5...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30:  50%|████▉     | 936/1875 [02:06<02:04,  7.57it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/30 - Midpoint, Average Loss: 3.4695\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/30: 100%|██████████| 1875/1875 [04:05<00:00,  7.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 6/30 completed, Final Average Loss: 3.4724\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30:  50%|████▉     | 936/1875 [02:04<02:06,  7.41it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/30 - Midpoint, Average Loss: 3.4758\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/30: 100%|██████████| 1875/1875 [04:08<00:00,  7.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 7/30 completed, Final Average Loss: 3.4723\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30:  50%|████▉     | 936/1875 [02:03<02:08,  7.30it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/30 - Midpoint, Average Loss: 3.4823\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/30: 100%|██████████| 1875/1875 [04:10<00:00,  7.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 8/30 completed, Final Average Loss: 3.4725\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30:  50%|████▉     | 937/1875 [02:03<02:06,  7.43it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/30 - Midpoint, Average Loss: 3.4737\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/30: 100%|██████████| 1875/1875 [04:06<00:00,  7.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 9/30 completed, Final Average Loss: 3.4704\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30:  50%|████▉     | 936/1875 [02:03<02:08,  7.28it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/30 - Midpoint, Average Loss: 3.4627\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/30: 100%|██████████| 1875/1875 [04:07<00:00,  7.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 10/30 completed, Final Average Loss: 3.4708\n\nSaving model to arabic_gpt_tpu_model_epoch_10...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30:  50%|████▉     | 937/1875 [02:03<02:02,  7.68it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/30 - Midpoint, Average Loss: 3.4726\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/30: 100%|██████████| 1875/1875 [04:19<00:00,  7.23it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 11/30 completed, Final Average Loss: 3.4709\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30:  50%|████▉     | 937/1875 [02:15<02:13,  7.04it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/30 - Midpoint, Average Loss: 3.4750\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/30: 100%|██████████| 1875/1875 [04:31<00:00,  6.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 12/30 completed, Final Average Loss: 3.4703\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30:  50%|████▉     | 937/1875 [02:17<02:07,  7.33it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/30 - Midpoint, Average Loss: 3.4789\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/30: 100%|██████████| 1875/1875 [04:35<00:00,  6.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 13/30 completed, Final Average Loss: 3.4714\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/30:  50%|████▉     | 937/1875 [02:18<02:15,  6.90it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/30 - Midpoint, Average Loss: 3.4735\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/30: 100%|██████████| 1875/1875 [04:35<00:00,  6.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 14/30 completed, Final Average Loss: 3.4730\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/30:  50%|████▉     | 937/1875 [02:17<02:12,  7.09it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/30 - Midpoint, Average Loss: 3.4592\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/30: 100%|██████████| 1875/1875 [04:34<00:00,  6.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 15/30 completed, Final Average Loss: 3.4715\n\nSaving model to arabic_gpt_tpu_model_epoch_15...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/30:  50%|████▉     | 937/1875 [02:15<02:11,  7.13it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16/30 - Midpoint, Average Loss: 3.4701\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/30: 100%|██████████| 1875/1875 [04:31<00:00,  6.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 16/30 completed, Final Average Loss: 3.4709\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/30:  50%|████▉     | 937/1875 [02:16<02:09,  7.25it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17/30 - Midpoint, Average Loss: 3.4719\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/30: 100%|██████████| 1875/1875 [04:33<00:00,  6.86it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 17/30 completed, Final Average Loss: 3.4697\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/30:  50%|████▉     | 937/1875 [02:16<02:15,  6.95it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18/30 - Midpoint, Average Loss: 3.4738\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/30: 100%|██████████| 1875/1875 [04:32<00:00,  6.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 18/30 completed, Final Average Loss: 3.4698\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/30:  50%|████▉     | 937/1875 [02:16<02:00,  7.81it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19/30 - Midpoint, Average Loss: 3.4761\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/30: 100%|██████████| 1875/1875 [04:21<00:00,  7.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 19/30 completed, Final Average Loss: 3.4716\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/30:  50%|████▉     | 937/1875 [02:06<02:00,  7.78it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20/30 - Midpoint, Average Loss: 3.4811\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/30: 100%|██████████| 1875/1875 [04:17<00:00,  7.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 20/30 completed, Final Average Loss: 3.4709\n\nSaving model to arabic_gpt_tpu_model_epoch_20...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30:  50%|████▉     | 937/1875 [02:14<02:08,  7.29it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 21/30 - Midpoint, Average Loss: 3.4699\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 21/30: 100%|██████████| 1875/1875 [04:32<00:00,  6.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 21/30 completed, Final Average Loss: 3.4704\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/30:  50%|████▉     | 937/1875 [02:16<02:10,  7.17it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 22/30 - Midpoint, Average Loss: 3.4744\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 22/30: 100%|██████████| 1875/1875 [04:33<00:00,  6.85it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 22/30 completed, Final Average Loss: 3.4711\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/30:  50%|████▉     | 937/1875 [02:16<02:15,  6.92it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 23/30 - Midpoint, Average Loss: 3.4684\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 23/30: 100%|██████████| 1875/1875 [04:32<00:00,  6.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 23/30 completed, Final Average Loss: 3.4714\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/30:  50%|████▉     | 937/1875 [02:15<02:01,  7.72it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 24/30 - Midpoint, Average Loss: 3.4695\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 24/30: 100%|██████████| 1875/1875 [04:29<00:00,  6.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 24/30 completed, Final Average Loss: 3.4716\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/30:  50%|████▉     | 937/1875 [02:22<02:24,  6.48it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 25/30 - Midpoint, Average Loss: 3.4794\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 25/30: 100%|██████████| 1875/1875 [04:44<00:00,  6.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 25/30 completed, Final Average Loss: 3.4705\n\nSaving model to arabic_gpt_tpu_model_epoch_25...\nModel saved successfully!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/30:  50%|████▉     | 937/1875 [02:07<02:03,  7.58it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 26/30 - Midpoint, Average Loss: 3.4818\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 26/30: 100%|██████████| 1875/1875 [04:16<00:00,  7.31it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 26/30 completed, Final Average Loss: 3.4710\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/30:  50%|████▉     | 937/1875 [02:08<02:09,  7.26it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 27/30 - Midpoint, Average Loss: 3.4802\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 27/30: 100%|██████████| 1875/1875 [04:17<00:00,  7.29it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 27/30 completed, Final Average Loss: 3.4711\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/30:  50%|████▉     | 937/1875 [02:07<02:03,  7.59it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 28/30 - Midpoint, Average Loss: 3.4610\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 28/30: 100%|██████████| 1875/1875 [04:09<00:00,  7.52it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 28/30 completed, Final Average Loss: 3.4712\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/30:  50%|████▉     | 936/1875 [01:59<02:07,  7.39it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 29/30 - Midpoint, Average Loss: 3.4726\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 29/30: 100%|██████████| 1875/1875 [03:59<00:00,  7.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 29/30 completed, Final Average Loss: 3.4697\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/30:  50%|████▉     | 936/1875 [01:59<02:06,  7.42it/s]","output_type":"stream"},{"name":"stdout","text":"\nEpoch 30/30 - Midpoint, Average Loss: 3.4722\n\n","output_type":"stream"},{"name":"stderr","text":"Epoch 30/30: 100%|██████████| 1875/1875 [03:59<00:00,  7.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"\nEpoch 30/30 completed, Final Average Loss: 3.4708\n\nSaving model to arabic_gpt_tpu_model_epoch_30...\nModel saved successfully!\n\nTesting predictions with the trained model:\n\nInput sentence: مرحبا كيف حالك\nPredictions:\n يا: 0.8551\n�: 0.0441\n في: 0.0376\n إذا: 0.0344\n أيها: 0.0288\n\nInput sentence: انا اريد ان اتعلم\nPredictions:\n من: 0.2998\n الفرنسية: 0.2593\n شيئا: 0.2137\n كيف: 0.1600\n أن: 0.0671\n\nInput sentence: شكرا جزيلا على\nPredictions:\n هذا: 0.2444\n هذه: 0.2081\n حسن: 0.2038\n جهود: 0.1719\n ما: 0.1718\n\nInput sentence: السلام عليكم ورحمة\nPredictions:\n الله: 0.9915\n الباري: 0.0028\n الرحمن: 0.0026\n ربي: 0.0018\n لله: 0.0013\n\nInput sentence: اهلا وسهلا بكم في\nPredictions:\n موقع: 0.3632\n هذا: 0.2535\n هذه: 0.1682\n منتدى: 0.1108\n حلقة: 0.1042\n\nInput sentence: انا احب القراءة\nPredictions:\n كثيرا: 0.3133\n والكتابة: 0.2417\n�: 0.2235\n في: 0.1192\n لكن: 0.1023\n\nInput sentence: هذا الكتاب جميل\nPredictions:\n جدا: 0.4483\n في: 0.1922\n ومفيد: 0.1303\n للقارئ: 0.1189\n لكنه: 0.1103\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive(\n    '/kaggle/working/test2',  # Output ZIP file path (no .zip extension)\n    'zip',                                            # Archive format\n    root_dir='/kaggle/working',                       # Base directory\n    base_dir='arabic_gpt_tpu_model_epoch_30'          # Folder to zip (relative to root_dir)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-11T22:17:29.520959Z","iopub.execute_input":"2025-05-11T22:17:29.521324Z","iopub.status.idle":"2025-05-11T22:18:05.788215Z","shell.execute_reply.started":"2025-05-11T22:17:29.521296Z","shell.execute_reply":"2025-05-11T22:18:05.781836Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/test2.zip'"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}